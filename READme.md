# Human Variability vs. Machine Consistency: A Linguistic Analysis of Texts Generated by Humans and Large Language Models

This repository accompanies the paper *Human Variability vs. Machine Consistency: A Linguistic Analysis of Texts Generated by Humans and Large Language Models*. It provides the code and resources necessary to conduct the analyses described in the paper.

## Important Instructions

1. **Download Data**  
   Before beginning, download Subtask B of SemEval 169 2024 Task 8 from this [GitHub repository](https://github.com/mbzuai-nlp/SemEval2024-task8). This dataset is required to perform the analyses.

2. **Initial Setup**  
   After downloading the dataset, run `lftk_calculation.py`. This script generates the essential CSV file needed for the analyses in other sections of the repository.

3. **Feature Calculation and Analysis**  
   Each major feature set has its own folder with specific instructions for running the calculations and analyses:
   
   - **lftkfeatures**  
     - Run `lftk_analysis.ipynb` after generating the CSV with `lftk_calculation.py`.

   - **emotionality**  
     - Inside this folder, you’ll find two files:
       - `emotionality_calculation.py`: Run this first to calculate emotionality scores.
       - `emotionality_analysis.ipynb`: After calculating scores, open this notebook to analyze the results.

   - **semanticdistanceandsyntacticdepth**  
     - Inside this folder, you’ll find three files:
       - `semanticdistance_calculation.py`: Run this to compute semantic distance.
       - `syntacticdepth_calculation.py`: Run this to calculate syntactic depth.
       - `semanticdistanceandsyntacticdepth_analysis.ipynb`: After running both calculations, use this notebook for analysis, which includes plots of semantic distance, syntactic depth, and unique word counts.

Each folder contains additional documentation within the `.ipynb` and `.py` files to guide you through specific analyses. Please ensure that you run the `.py` files first to calculate features before opening the `.ipynb` notebooks for detailed analysis.

4. **Classification**  
   The `classification` folder contains the classification test on Authorship Attribution task. This test aims to identify and analyze patterns in texts generated by humans and large language models to determine authorship characteristics.

